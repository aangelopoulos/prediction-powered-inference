{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ebc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.optimize import brentq\n",
    "from scipy.stats import binom\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541f1b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calib acc: 0.962313466496375\n",
      "Test acc: 0.9789407941012395\n"
     ]
    }
   ],
   "source": [
    "# Get data 2006-2014 from the following link: https://darchive.mblwhoilibrary.org/handle/1912/7341\n",
    "# Unzip and merge the datasets in the following directory\n",
    "calib_data = np.load('../calib-outputs.npz')\n",
    "test_data = np.load('../test-outputs.npz')\n",
    "calib_preds = calib_data['preds'].astype(int)\n",
    "calib_labels = calib_data['labels'].astype(int)\n",
    "test_preds = test_data['preds'].astype(int)\n",
    "test_labels = test_data['labels'].astype(int)\n",
    "classes = np.load('../classes.npy')\n",
    "num_classes = classes.shape[0]\n",
    "\n",
    "plankton_classes = np.isin(classes,['mix','mix_elongated','detritus','bad', 'bead', 'bubble', 'other_interaction', 'pollen', 'spore'],invert=True)\n",
    "plankton_classes_list = np.array(np.where(plankton_classes)[0])\n",
    "\n",
    "true_count = np.isin(test_labels, plankton_classes_list).sum()\n",
    "uncorrected_est = np.isin(test_preds, plankton_classes_list).sum()\n",
    "\n",
    "calib_preds = np.isin(calib_preds, plankton_classes_list)\n",
    "calib_labels = np.isin(calib_labels, plankton_classes_list)\n",
    "test_preds = np.isin(test_preds, plankton_classes_list)\n",
    "test_labels = np.isin(test_labels, plankton_classes_list)\n",
    "\n",
    "print(f\"Calib acc: {(calib_preds == calib_labels).astype(int).mean()}\")\n",
    "print(f\"Test acc: {(test_preds == test_labels).astype(int).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a8ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the unique classes \n",
    "calib_uq, calib_uq_counts = np.unique(calib_labels, return_counts=True)\n",
    "calib_uq_freq = calib_uq_counts/calib_uq_counts.sum()\n",
    "calib_uq_sort = np.argsort(calib_uq_freq)[::-1]\n",
    "calib_uq_freq = calib_uq_freq[calib_uq_sort]; calib_uq = calib_uq[calib_uq_sort];\n",
    "calib_uq_cumsum = np.cumsum(calib_uq_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7d91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "alpha = 0.1\n",
    "delta_1 = 0.099\n",
    "delta_2 = 0.001\n",
    "nu = np.array([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283a88f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the confusion matrix\n",
    "n = calib_preds.shape[0]\n",
    "N = test_preds.shape[0]\n",
    "\n",
    "C = np.zeros((2,2)).astype(int)\n",
    "for j in range(2):\n",
    "    for l in range(2):\n",
    "        C[j,l] = np.bitwise_and(calib_preds == j,calib_labels == l).astype(int).sum()\n",
    "Ahat = C / C.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1fb7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98  0.152]\n",
      " [0.02  0.848]]\n",
      "[[ 1.024 -0.184]\n",
      " [-0.024  1.184]]\n"
     ]
    }
   ],
   "source": [
    "# Construct Ahat\n",
    "Ahatinv = np.linalg.inv(Ahat)\n",
    "print(np.array_str(Ahat, precision=3))\n",
    "print(np.array_str(Ahatinv, precision=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea124bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the point estimate\n",
    "target_uq, target_uq_counts = np.unique(test_preds, return_counts=True)\n",
    "target_uq_freq = target_uq_counts/target_uq_counts.sum()\n",
    "target_uq_sort = np.argsort(target_uq_freq)[::-1]\n",
    "target_uq_freq = target_uq_freq[target_uq_sort]; target_uq = target_uq[target_uq_sort];\n",
    "qfhat = target_uq_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7679425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MAI\n",
    "nmin = int(calib_uq_freq.min()*n)\n",
    "point_estimate = nu@Ahatinv@qfhat\n",
    "\n",
    "term1 = np.abs(nu@Ahatinv).sum()*np.sqrt(2/nmin*np.log(6/delta_1))\n",
    "term2 = np.abs(nu@Ahatinv).sum()*np.sqrt(2/N*np.log(6/delta_1))\n",
    "\n",
    "qyhat_lb = np.maximum(np.minimum(point_estimate - term1 - term2,1),0)\n",
    "qyhat_ub = np.maximum(np.minimum(point_estimate + term1 + term2,1),0)\n",
    "\n",
    "count_plankton_lb = int(binom.ppf(delta_2, N, qyhat_lb))\n",
    "count_plankton_ub = int(binom.ppf(1-delta_2, N, qyhat_ub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2fde832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rectified confidence interval for the number of plankton observed in 2014 is [11562,25962] ([3.6%,7.7%]).\n",
      "The true number of plankton observed in 2014 was 23538 (7.1%), which lies in the interval.\n",
      "The uncorrected estimate was 22068 (6.7%).\n"
     ]
    }
   ],
   "source": [
    "print(f\"The rectified confidence interval for the number of plankton observed in 2014 is [{count_plankton_lb},{count_plankton_ub}] ([{100*qyhat_lb:.1f}%,{100*qyhat_ub:.1f}%]).\")\n",
    "print(f\"The true number of plankton observed in 2014 was {true_count} ({true_count/N*100:.1f}%), which lies in the interval.\")\n",
    "print(f\"The uncorrected estimate was {uncorrected_est} ({uncorrected_est/N*100:.1f}%).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4770e36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The uncorrected confidence interval for the number of plankton observed in 2014 is [21014,23136] ([6.5%,6.9%]).\n",
      "The i.i.d. confidence interval for the number of plankton observed in 2014 is [21014,23136] ([5.7%,8.3%]).\n"
     ]
    }
   ],
   "source": [
    "# Baselines\n",
    "\n",
    "# Naive intervals\n",
    "naive_epsilon = np.sqrt(1/(2*N) * np.log(1/delta_1))\n",
    "naive_lb = qfhat[1] - naive_epsilon\n",
    "naive_ub = qfhat[1] + naive_epsilon\n",
    "naive_count_lb = int(binom.ppf(delta_2, N, naive_lb))\n",
    "naive_count_ub = int(binom.ppf(1-delta_2, N, naive_ub))\n",
    "print(f\"The uncorrected confidence interval for the number of plankton observed in 2014 is [{naive_count_lb},{naive_count_ub}] ([{100*naive_lb:.1f}%,{100*naive_ub:.1f}%]).\")\n",
    "\n",
    "# IID intervals\n",
    "\n",
    "iid_epsilon = np.sqrt(2)*(2*np.sqrt(np.log(3/delta_1)/n) + np.sqrt(np.log(3/delta_1)/N))\n",
    "bias_estimate = (calib_preds.astype(float) - calib_labels.astype(float)).mean()\n",
    "iid_lb = qfhat[1] - bias_estimate - iid_epsilon\n",
    "iid_ub = qfhat[1] - bias_estimate + iid_epsilon\n",
    "iid_count_lb = int(binom.ppf(delta_2, N, naive_lb))\n",
    "iid_count_ub = int(binom.ppf(1-delta_2, N, naive_ub))\n",
    "print(f\"The i.i.d. confidence interval for the number of plankton observed in 2014 is [{iid_count_lb},{iid_count_ub}] ([{100*iid_lb:.1f}%,{100*iid_ub:.1f}%]).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

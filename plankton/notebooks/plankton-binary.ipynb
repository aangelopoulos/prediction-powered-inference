{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ebc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, copy\n",
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import brentq\n",
    "from scipy.stats import norm, binom\n",
    "from concentration import linfty_dkw, linfty_binom, wsr_iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541f1b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calib acc: 0.962313466496375\n",
      "Test acc: 0.9789407941012395\n"
     ]
    }
   ],
   "source": [
    "# Get data 2006-2014 from the following link: https://darchive.mblwhoilibrary.org/handle/1912/7341\n",
    "# Unzip and merge the datasets in the following directory\n",
    "calib_data = np.load('../calib-outputs.npz')\n",
    "test_data = np.load('../test-outputs.npz')\n",
    "calib_preds = calib_data['preds'].astype(int)\n",
    "calib_labels = calib_data['labels'].astype(int)\n",
    "test_preds = test_data['preds'].astype(int)\n",
    "test_labels = test_data['labels'].astype(int)\n",
    "classes = np.load('../classes.npy')\n",
    "num_classes = classes.shape[0]\n",
    "\n",
    "plankton_classes = np.isin(classes,['mix','mix_elongated','detritus','bad', 'bead', 'bubble', 'other_interaction', 'pollen', 'spore'],invert=True)\n",
    "plankton_classes_list = np.array(np.where(plankton_classes)[0])\n",
    "\n",
    "true_count = np.isin(test_labels, plankton_classes_list).sum()\n",
    "N = test_labels.shape[0]\n",
    "true_frac = true_count/N\n",
    "uncorrected_est = np.isin(test_preds, plankton_classes_list).sum()\n",
    "\n",
    "calib_preds = np.isin(calib_preds, plankton_classes_list)\n",
    "calib_labels = np.isin(calib_labels, plankton_classes_list)\n",
    "test_preds = np.isin(test_preds, plankton_classes_list)\n",
    "test_labels = np.isin(test_labels, plankton_classes_list)\n",
    "\n",
    "print(f\"Calib acc: {(calib_preds == calib_labels).astype(int).mean()}\")\n",
    "print(f\"Test acc: {(test_preds == test_labels).astype(int).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a8ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the unique classes \n",
    "calib_uq, calib_uq_counts = np.unique(calib_labels, return_counts=True)\n",
    "calib_uq_freq = calib_uq_counts/calib_uq_counts.sum()\n",
    "calib_uq_sort = np.argsort(calib_uq_freq)[::-1]\n",
    "calib_uq_freq = calib_uq_freq[calib_uq_sort]; calib_uq = calib_uq[calib_uq_sort];\n",
    "calib_uq_cumsum = np.cumsum(calib_uq_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7d91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "alpha = 0.05\n",
    "delta = 0.04\n",
    "K = 2\n",
    "nu = np.array([0,1])\n",
    "n_max = calib_preds.shape[0]\n",
    "ns = np.linspace(1000, n_max, 100)\n",
    "num_trials = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9a9f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_qfhat(test_preds):\n",
    "    # Construct the point estimate\n",
    "    target_uq, target_uq_counts = np.unique(test_preds, return_counts=True)\n",
    "    target_uq_freq = target_uq_counts/target_uq_counts.sum()\n",
    "    target_uq_sort = np.argsort(target_uq_freq)[::-1]\n",
    "    target_uq_freq = target_uq_freq[target_uq_sort]; target_uq = target_uq[target_uq_sort];\n",
    "    qfhat = target_uq_freq\n",
    "    return qfhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cd8166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveML(test_preds, alpha, delta):\n",
    "    N = test_preds.shape[0]\n",
    "    qfhat = form_qfhat(test_preds)\n",
    "    naive_epsilon = np.sqrt(1/(2*N) * np.log(1/delta))\n",
    "    naive_lb = qfhat[1] - naive_epsilon\n",
    "    naive_ub = qfhat[1] + naive_epsilon\n",
    "    naive_count_lb = int(binom.ppf(alpha-delta, N, naive_lb))\n",
    "    naive_count_ub = int(binom.ppf(1-(alpha-delta), N, naive_ub))\n",
    "    return [naive_count_lb, naive_count_ub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28f28b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppi_iid(calib_preds, test_preds, calib_labels, alpha, delta):\n",
    "    N = test_preds.shape[0]\n",
    "    qfhat = form_qfhat(test_preds)\n",
    "\n",
    "    bias_estimate = (calib_preds.astype(float) - calib_labels.astype(float)).mean()\n",
    "    grid = np.linspace(0.48,0.52,1000)\n",
    "    step = calib_preds.shape[0]\n",
    "    possible_biases = bias_estimate + 2*wsr_iid((calib_preds.astype(float) - calib_labels.astype(float)+1)/2, delta, grid, intersection=False)-1\n",
    "    iid_lb = qfhat[1] + possible_biases.min()\n",
    "    iid_ub = qfhat[1] + possible_biases.max()\n",
    "    iid_count_lb = int(binom.ppf(alpha-delta, N, iid_lb))\n",
    "    iid_count_ub = int(binom.ppf(1-(alpha-delta), N, iid_ub))\n",
    "    return [iid_count_lb, iid_count_ub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cb82c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppi_label_shift(calib_preds, test_preds, calib_labels, K, alpha, delta):\n",
    "    # Construct the confusion matrix\n",
    "    n = calib_preds.shape[0]\n",
    "    N = test_preds.shape[0]\n",
    "\n",
    "    # Construct column-normalized confusion matrix Ahat\n",
    "    C = np.zeros((K,K)).astype(int)\n",
    "    for j in range(K):\n",
    "        for l in range(K):\n",
    "            C[j,l] = np.bitwise_and(calib_preds == j,calib_labels == l).astype(int).sum()\n",
    "    Ahat = C / C.sum(axis=0)\n",
    "    # Invert Ahat\n",
    "    Ahatinv = np.linalg.inv(Ahat)\n",
    "    qfhat = form_qfhat(test_preds)\n",
    "    # Calculate the bound\n",
    "    point_estimate = nu@Ahatinv@qfhat\n",
    "\n",
    "    nmin = C.sum(axis=0).min()\n",
    "\n",
    "    def invert_theta(theta): return np.sqrt(1/(4*nmin))*(norm.ppf(1-(theta*delta)/(2*K**2)) - norm.ppf((theta*delta)/(2*K**2))) - np.sqrt(2/N*np.log(2/((1-theta)*delta)))\n",
    "    theta = brentq(invert_theta,1e-9,1-1e-9)\n",
    "    epsilon1 = max([linfty_binom(C.sum(axis=0)[k], K, theta*delta, Ahat[:,k]) for k in range(K)])\n",
    "    epsilon2 = linfty_dkw(N,K,(1-theta)*delta)\n",
    "\n",
    "    qyhat_lb = np.clip(point_estimate - epsilon1 - epsilon2, 0, 1)\n",
    "    qyhat_ub = np.clip(point_estimate + epsilon1 + epsilon2, 0, 1)\n",
    "\n",
    "    count_plankton_lb = int(binom.ppf(alpha-delta, N, qyhat_lb))\n",
    "    count_plankton_ub = int(binom.ppf(1-(alpha-delta), N, qyhat_ub))\n",
    "    return [count_plankton_lb, count_plankton_ub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e43c7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive\n",
    "naive_lb, naive_ub = naiveML(test_preds, alpha, delta)\n",
    "naive_fraclb, naive_fracub = naive_lb/N, naive_ub/N\n",
    "# Without label shift\n",
    "ppi_iid_lb, ppi_iid_ub = ppi_iid(calib_preds, test_preds, calib_labels, alpha, delta)\n",
    "ppi_iid_fraclb, ppi_iid_fracub = ppi_iid_lb/N, ppi_iid_ub/N\n",
    "# With label shift\n",
    "ppi_ls_lb, ppi_ls_ub = ppi_label_shift(calib_preds, test_preds, calib_labels, K, alpha, delta)\n",
    "ppi_ls_fraclb, ppi_ls_fracub = ppi_ls_lb/N, ppi_ls_ub/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de6ee6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true number of plankton observed in 2014 was 23538 (7.1%).\n",
      "The naive baseline for the number of plankton observed in 2014 is [21011,23136] ([6.4%,7.0%]).\n",
      "The prediction-powered baseline (no label shift) for the number of plankton observed in 2014 is [19497,20637] ([5.9%,6.3%]).\n",
      "The prediction-powered confidence interval for the number of plankton observed in 2014 is [13670,23785] ([4.1%,7.2%]).\n"
     ]
    }
   ],
   "source": [
    "print(f\"The true number of plankton observed in 2014 was {true_count} ({true_count/N*100:.1f}%).\")\n",
    "print(f\"The naive baseline for the number of plankton observed in 2014 is [{naive_lb},{naive_ub}] ([{100*naive_fraclb:.1f}%,{100*naive_fracub:.1f}%]).\")\n",
    "print(f\"The prediction-powered baseline (no label shift) for the number of plankton observed in 2014 is [{ppi_iid_lb},{ppi_iid_ub}] ([{100*ppi_iid_fraclb:.1f}%,{100*ppi_iid_fracub:.1f}%]).\")\n",
    "print(f\"The prediction-powered confidence interval for the number of plankton observed in 2014 is [{ppi_ls_lb},{ppi_ls_ub}] ([{100*ppi_ls_fraclb:.1f}%,{100*ppi_ls_fracub:.1f}%]).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
